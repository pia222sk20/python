{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef71bd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from requests->kagglehub) (2025.10.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata2\\miniconda3\\envs\\deep\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\playdata2\\miniconda3\\envs\\deep\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\playdata2\\.cache\\kagglehub\\datasets\\aashita\\nyt-comments\\versions\\13\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcb8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "article_lists = glob(path+'/*.*',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1cdf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>716</td>\n",
       "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
       "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-01 00:15:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>One of the largest photo displays in Times his...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def3237c459f24986d7c84</td>\n",
       "      <td>823</td>\n",
       "      <td>By GAIL COLLINS</td>\n",
       "      <td>article</td>\n",
       "      <td>And Now,  the Dreaded Trump Curse</td>\n",
       "      <td>['United States Politics and Government', 'Tru...</td>\n",
       "      <td>3</td>\n",
       "      <td>OpEd</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-04-01 00:23:58</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Meet the gang from under the bus.</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Op-Ed</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def9f57c459f24986d7c90</td>\n",
       "      <td>575</td>\n",
       "      <td>By THE EDITORIAL BOARD</td>\n",
       "      <td>article</td>\n",
       "      <td>Venezuela’s Descent Into Dictatorship</td>\n",
       "      <td>['Venezuela', 'Politics and Government', 'Madu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-04-01 00:53:06</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>A court ruling annulling the legislature’s aut...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58defd317c459f24986d7c95</td>\n",
       "      <td>1374</td>\n",
       "      <td>By MICHAEL POWELL</td>\n",
       "      <td>article</td>\n",
       "      <td>Stain Permeates Basketball Blue Blood</td>\n",
       "      <td>['Basketball (College)', 'University of North ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-01 01:06:52</td>\n",
       "      <td>College Basketball</td>\n",
       "      <td>For two decades, until 2013, North Carolina en...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/sports/ncaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58df09b77c459f24986d7ca7</td>\n",
       "      <td>708</td>\n",
       "      <td>By DEB AMLEN</td>\n",
       "      <td>article</td>\n",
       "      <td>Taking Things for Granted</td>\n",
       "      <td>['Crossword Puzzles']</td>\n",
       "      <td>3</td>\n",
       "      <td>Games</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-01 02:00:14</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>In which Howard Barkin and Will Shortz teach u...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/crosswords/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abstract                 articleID  articleWordCount  \\\n",
       "0      NaN  58def1347c459f24986d7c80               716   \n",
       "1      NaN  58def3237c459f24986d7c84               823   \n",
       "2      NaN  58def9f57c459f24986d7c90               575   \n",
       "3      NaN  58defd317c459f24986d7c95              1374   \n",
       "4      NaN  58df09b77c459f24986d7ca7               708   \n",
       "\n",
       "                                byline documentType  \\\n",
       "0  By STEPHEN HILTNER and SUSAN LEHMAN      article   \n",
       "1                      By GAIL COLLINS      article   \n",
       "2               By THE EDITORIAL BOARD      article   \n",
       "3                    By MICHAEL POWELL      article   \n",
       "4                         By DEB AMLEN      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Finding an Expansive View  of a Forgotten Peop...   \n",
       "1                  And Now,  the Dreaded Trump Curse   \n",
       "2              Venezuela’s Descent Into Dictatorship   \n",
       "3              Stain Permeates Basketball Blue Blood   \n",
       "4                          Taking Things for Granted   \n",
       "\n",
       "                                            keywords  multimedia    newDesk  \\\n",
       "0  ['Photography', 'New York Times', 'Niger', 'Fe...           3    Insider   \n",
       "1  ['United States Politics and Government', 'Tru...           3       OpEd   \n",
       "2  ['Venezuela', 'Politics and Government', 'Madu...           3  Editorial   \n",
       "3  ['Basketball (College)', 'University of North ...           3     Sports   \n",
       "4                              ['Crossword Puzzles']           3      Games   \n",
       "\n",
       "   printPage              pubDate         sectionName  \\\n",
       "0          2  2017-04-01 00:15:41             Unknown   \n",
       "1         23  2017-04-01 00:23:58             Unknown   \n",
       "2         22  2017-04-01 00:53:06             Unknown   \n",
       "3          1  2017-04-01 01:06:52  College Basketball   \n",
       "4          0  2017-04-01 02:00:14             Unknown   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  One of the largest photo displays in Times his...  The New York Times   \n",
       "1                  Meet the gang from under the bus.  The New York Times   \n",
       "2  A court ruling annulling the legislature’s aut...  The New York Times   \n",
       "3  For two decades, until 2013, North Carolina en...  The New York Times   \n",
       "4  In which Howard Barkin and Will Shortz teach u...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2017/03/31/insider/nig...  \n",
       "1          Op-Ed  https://www.nytimes.com/2017/03/31/opinion/and...  \n",
       "2      Editorial  https://www.nytimes.com/2017/03/31/opinion/ven...  \n",
       "3           News  https://www.nytimes.com/2017/03/31/sports/ncaa...  \n",
       "4           News  https://www.nytimes.com/2017/03/31/crosswords/...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(article_lists[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4b800",
   "metadata": {},
   "source": [
    "##### LSTM\n",
    "- 입력 : \"나는 파이썬을 좋아합니다. 따라서 나는 ___ 을 잘합니다.\"\n",
    "- 일반신경망 : 공부  ( 파이썬 정보가 희석... 잊어)\n",
    "- LSTM : 프로그래밍(오래된 정보도 기억)\n",
    "\n",
    "- 핵심 키워드\n",
    "    - 장기기억 : 중요한 정보는 오래기억\n",
    "    - 단기기억 : 불필요한 정보는 버림\n",
    "    - 순서이해 : 시간순서 이해\n",
    "\n",
    "##### 3개의 Gate를 통해 정보의 흐름을 제어\n",
    " - LSTM 셀 (한 시점 t)\n",
    "    - 입력 : $x_t$ (현재데이터)\n",
    "    - 이전 은닉상태 : $h_{t-1}$\n",
    "    - 이전 셀 상태 : $c_{t-1}$\n",
    "\n",
    "    --> forget gate --> input date   --> output gate\n",
    "        잊을데이터        추가할 데이터     출력할 데이터\n",
    "    \n",
    "    출력 $h_t$ ,  $c_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702adbf",
   "metadata": {},
   "source": [
    "Forget Gate(잊음 관문)\n",
    "\n",
    "\n",
    "$f_t$ = $s(w_f . [h_{t-1}, x_t ] + b_f )$\n",
    "\n",
    "s : sigmoid함수(0~1)\n",
    "\n",
    "이전 셀상태 : [1.5, -0.3, 2.1]\n",
    "현재입력 : '새로운 문장 입력'\n",
    "\n",
    "$f_t$ = [0.1,0.05,0.9]\n",
    "\n",
    "결과 : [ 1.5*0.1, -0.3*0.05, 2.1*0.9  ]  =  [0.15, -0.015, 1.89]\n",
    "\n",
    "첫 2개는 버리고 3번째는 유지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937c4b4",
   "metadata": {},
   "source": [
    "##### input gate : 입력\n",
    "- 첫 번째는 70%받고 두번째는 30% 받음\n",
    "\n",
    "##### Cell State 업데이트\n",
    " - 이전기억에서 필요한 것만 유지하고 새로운 정보에서 필요한 것만 유지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84d41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455d6281",
   "metadata": {},
   "source": [
    "input 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4e0a9",
   "metadata": {},
   "source": [
    "시점 t에서:\n",
    "- $x_t$      : 현재 입력 데이터 (벡터)\n",
    "- $h_{t-1}$  : 이전 시점의 은닉 상태 (벡터)\n",
    "- $C_{t-1}$  : 이전 시점의 셀 상태 (벡터) ← 장기 기억!\n",
    "- $W_*$, $U_*$ : 가중치 행렬\n",
    "- $b_*$      : 편향 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa5e5b",
   "metadata": {},
   "source": [
    "- $x_t$ = [0.2, -0.5, 0.8]      (3개 입력 피처)\n",
    "- $h_{t-1}$ = [0.1, 0.3, -0.2, 0.5]  (4개 은닉)\n",
    "- $C_{t-1}$ = [0.4, -0.1, 0.6, 0.2]  (4개 셀 상태)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009bbb3",
   "metadata": {},
   "source": [
    "Forget Gate ( $f_t$ ) - 어제 기억을 얼마나 유지할까\n",
    "- $f_t$ = $σ( W_f · [h_{t-1}, x_t] + b_f )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7bac5",
   "metadata": {},
   "source": [
    "1단계: $h_{t-1}$ 과 $x_t$ 를 연결 (concatenate)\n",
    "\n",
    "   $[ h_{t-1}, x_t]$ = [0.1,    0.3,    -0.2,    0.5,    0.2,    -0.5,    0.8]\n",
    "\n",
    "               $h_{t-1}$    $x_t$\n",
    "\n",
    "                        4개              3개\n",
    "\n",
    "                               ↓\n",
    "\n",
    "                           총 7개 벡터\n",
    "\n",
    "2단계: 가중치 행렬 곱하기\n",
    "   $W_f · [h_{t-1}, x_t]$\n",
    "   \n",
    "   $W_f$ 는 크기: (4, 7) 행렬\n",
    "   (왜 (4, 7)? → 은닉 크기 4개, 입력 7개)\n",
    "   \n",
    "   결과: 4개의 값\n",
    "\n",
    "3단계: 편향 더하기\n",
    "   + $b_f$  (크기: 4)\n",
    "   \n",
    "   결과: 4개의 값\n",
    "\n",
    "4단계: Sigmoid 함수 적용\n",
    "   $σ(x)$ = $\\frac{1}{1 + e^{-x}}$ \n",
    "   \n",
    "   이 함수는 모든 값을 0~1 사이로 변환!\n",
    "   \n",
    "   $f_t = [0.3, 0.8, 0.1, 0.9]$\n",
    "   \n",
    "   의미:\n",
    "   - 첫 번째 셀 상태: 30% 유지 (70% 잊음)\n",
    "   - 두 번째 셀 상태: 80% 유지 (20% 잊음)\n",
    "   - 세 번째 셀 상태: 10% 유지 (90% 잊음)\n",
    "   - 네 번째 셀 상태: 90% 유지 (10% 잊음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef2e115",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'headline'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[32m      8\u001b[39m     df = pd.read_csv(a)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheadline\u001b[49m.values\n\u001b[32m     10\u001b[39m     all_headline.extend( a.headline.values )\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'headline'"
     ]
    }
   ],
   "source": [
    "all_headline = []\n",
    "articles = [path for path in article_lists if \"Articles\" in path]\n",
    "# headline 정보만 추출 all_headline에 추가\n",
    "# 전처리 : 소문자로 변경하고 특수문자 제거\n",
    "import string\n",
    "\n",
    "for a in articles:\n",
    "    df = pd.read_csv(a)\n",
    "    a.headline.values\n",
    "    all_headline.extend( a.headline.values )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(article_lists[0])\n",
    "cleaned_sentence = [doc.lower() for doc in df.headline.values if doc not in string.punctuation ]\n",
    "\n",
    "\n",
    "# 모든 문장의 단어를 추출해 고유 번호 지정\n",
    "bow = {}\n",
    "for line in cleaned_sentence:\n",
    "    for w in line.split():\n",
    "        if w not in bow:\n",
    "            bow[w] = len(bow.keys())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7a7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 29, 30, 31]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bow[w] for w in cleaned_sentence[5].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bce2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0dddd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ed5f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Retrieving notices: done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\playdata2\\miniconda3\\envs\\deep\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipywidgets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ipywidgets-8.1.7           |     pyhd8ed1ab_0         112 KB  conda-forge\n",
      "    jupyterlab_widgets-3.0.15  |     pyhd8ed1ab_0         185 KB  conda-forge\n",
      "    widgetsnbextension-4.0.14  |     pyhd8ed1ab_0         868 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ipywidgets         conda-forge/noarch::ipywidgets-8.1.7-pyhd8ed1ab_0 \n",
      "  jupyterlab_widgets conda-forge/noarch::jupyterlab_widgets-3.0.15-pyhd8ed1ab_0 \n",
      "  widgetsnbextension conda-forge/noarch::widgetsnbextension-4.0.14-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "widgetsnbextension-4 | 868 KB    |            |   0% \n",
      "\n",
      "jupyterlab_widgets-3 | 185 KB    |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "ipywidgets-8.1.7     | 112 KB    |            |   0% \u001b[A\u001b[A\n",
      "\n",
      "jupyterlab_widgets-3 | 185 KB    | ########## | 100% \u001b[A\n",
      "\n",
      "\n",
      "ipywidgets-8.1.7     | 112 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "widgetsnbextension-4 | 868 KB    | ########## | 100% \n",
      "widgetsnbextension-4 | 868 KB    | ########## | 100% \n",
      "\n",
      "jupyterlab_widgets-3 | 185 KB    | ########## | 100% \u001b[A\n",
      "\n",
      "jupyterlab_widgets-3 | 185 KB    | ########## | 100% \u001b[A\n",
      "widgetsnbextension-4 | 868 KB    | ########## | 100% \n",
      "\n",
      "\n",
      "ipywidgets-8.1.7     | 112 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ipywidgets-8.1.7     | 112 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "# kaggle data download\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "\n",
    "# csv파일이 있는 경로 path\n",
    "csv_lists = glob(path+'/*.*')\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "    def __init__(self,csv_lists):\n",
    "        all_headlines = []\n",
    "\n",
    "        # 모든 헤드라인의 텍스트를 불러옴\n",
    "        for filename in csv_lists:\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "\n",
    "        # headline 중 unknown 값은 제거\n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "        \n",
    "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "\n",
    "        # 모든 문장의 단어를 추출해 고유번호 지정\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        # 모델의 입력으로 사용할 데이터\n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "\n",
    "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) \n",
    "                                        for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "\n",
    "        return seq\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])  # 입력 데이터\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)  # 출력 데이터\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d385004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array(2., dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextGeneration(csv_lists)\n",
    "x, y = next(iter(dataset))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17a37481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings):  # num_embeddings 전체 단어의 개수(어휘사전 크기)\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # 신경망이 이해할수 있도록 벡터로 변경\n",
    "        self.embed = nn.Embedding( num_embeddings=num_embeddings, embedding_dim=16)\n",
    "\n",
    "        # LSTM을 5개층  (배치, 시퀀스, 피처)  16 ~ 512\n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=64, num_layers=5,batch_first=True)\n",
    "        # 분류를위한 fc층  squence_length * 64 = 2*64  128\n",
    "        self.fc1 = nn.Linear( 128, num_embeddings)\n",
    "        self.fc2 = nn.Linear( num_embeddings , num_embeddings)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):  # 입력 (batch , sq_len) batch: 32  sq_len : 2\n",
    "        x = self.embed(x)  # 출력 (batch, sq_len, 16)  32 2 16\n",
    "\n",
    "        # lstm 모델 예측값\n",
    "        x, _ =  self.lstm(x) # 출력 (batch ,sq_len, 64)  32 2 64\n",
    "        x = torch.reshape(x, (x.shape[0], -1))  # 출력 (batch, sq_len x 64)  32, 128\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "027c16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2482"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d26444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]C:\\Users\\playdata2\\AppData\\Local\\Temp\\ipykernel_12464\\2990939988.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(data, dtype=torch.long))\n",
      "C:\\Users\\playdata2\\AppData\\Local\\Temp\\ipykernel_12464\\2990939988.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long))\n",
      "epoch:1 loss : 7.339003562927246: 100%|██████████| 125/125 [00:12<00:00,  9.83it/s] \n",
      "epoch:2 loss : 7.102578163146973: 100%|██████████| 125/125 [00:11<00:00, 11.08it/s] \n",
      "epoch:3 loss : 6.429633140563965: 100%|██████████| 125/125 [00:09<00:00, 12.93it/s] \n",
      "epoch:4 loss : 6.055781364440918: 100%|██████████| 125/125 [00:08<00:00, 14.30it/s] \n",
      "epoch:5 loss : 5.8805975914001465: 100%|██████████| 125/125 [00:08<00:00, 14.63it/s]\n",
      "epoch:6 loss : 5.789340496063232: 100%|██████████| 125/125 [00:08<00:00, 14.01it/s] \n",
      "epoch:7 loss : 5.679608345031738: 100%|██████████| 125/125 [00:08<00:00, 14.92it/s] \n",
      "epoch:8 loss : 5.8004841804504395: 100%|██████████| 125/125 [00:08<00:00, 14.64it/s]\n",
      "epoch:9 loss : 5.861344814300537: 100%|██████████| 125/125 [00:09<00:00, 13.61it/s] \n",
      "epoch:10 loss : 5.813143730163574: 100%|██████████| 125/125 [00:09<00:00, 12.84it/s] \n",
      "epoch:11 loss : 5.965473651885986: 100%|██████████| 125/125 [00:10<00:00, 12.03it/s] \n",
      "epoch:12 loss : 6.2779541015625: 100%|██████████| 125/125 [00:10<00:00, 12.10it/s]   \n",
      "epoch:13 loss : 6.096587181091309: 100%|██████████| 125/125 [00:10<00:00, 11.84it/s] \n",
      "epoch:14 loss : 5.942849159240723: 100%|██████████| 125/125 [00:10<00:00, 12.11it/s] \n",
      "epoch:15 loss : 5.896376132965088: 100%|██████████| 125/125 [00:10<00:00, 12.44it/s] \n",
      "epoch:16 loss : 6.704155921936035: 100%|██████████| 125/125 [00:10<00:00, 12.45it/s] \n",
      "epoch:17 loss : 6.258800983428955: 100%|██████████| 125/125 [00:09<00:00, 12.91it/s] \n",
      "epoch:18 loss : 6.327841758728027: 100%|██████████| 125/125 [00:10<00:00, 12.39it/s] \n",
      "epoch:19 loss : 6.403085231781006: 100%|██████████| 125/125 [00:10<00:00, 11.91it/s] \n",
      "epoch:20 loss : 5.835890769958496: 100%|██████████| 125/125 [00:11<00:00, 11.29it/s] \n",
      "epoch:21 loss : 6.089905738830566: 100%|██████████| 125/125 [00:10<00:00, 11.54it/s] \n",
      "epoch:22 loss : 6.333618640899658: 100%|██████████| 125/125 [00:10<00:00, 12.35it/s] \n",
      "epoch:23 loss : 6.194718837738037: 100%|██████████| 125/125 [00:10<00:00, 11.88it/s] \n",
      "epoch:24 loss : 6.173590183258057: 100%|██████████| 125/125 [00:10<00:00, 12.05it/s]\n",
      "epoch:25 loss : 6.111189842224121: 100%|██████████| 125/125 [00:09<00:00, 13.28it/s]\n",
      "epoch:26 loss : 5.791059970855713: 100%|██████████| 125/125 [00:10<00:00, 11.76it/s] \n",
      "epoch:27 loss : 5.6873884201049805: 100%|██████████| 125/125 [00:10<00:00, 12.04it/s]\n",
      "epoch:28 loss : 5.905057430267334: 100%|██████████| 125/125 [00:11<00:00, 11.26it/s] \n",
      "epoch:29 loss : 6.508025169372559: 100%|██████████| 125/125 [00:11<00:00, 11.23it/s] \n",
      "epoch:30 loss : 6.243862628936768: 100%|██████████| 125/125 [00:11<00:00, 11.36it/s] \n",
      "epoch:31 loss : 6.17256498336792: 100%|██████████| 125/125 [00:10<00:00, 11.46it/s]  \n",
      "epoch:32 loss : 6.121827602386475: 100%|██████████| 125/125 [00:10<00:00, 12.31it/s] \n",
      "epoch:33 loss : 6.0693254470825195: 100%|██████████| 125/125 [00:10<00:00, 11.78it/s]\n",
      "epoch:34 loss : 6.0375471115112305: 100%|██████████| 125/125 [00:10<00:00, 11.86it/s]\n",
      "epoch:35 loss : 6.039113998413086: 100%|██████████| 125/125 [00:09<00:00, 12.68it/s] \n",
      "epoch:36 loss : 6.003925800323486: 100%|██████████| 125/125 [00:09<00:00, 12.73it/s] \n",
      "epoch:37 loss : 5.930297374725342: 100%|██████████| 125/125 [00:09<00:00, 12.77it/s] \n",
      "epoch:38 loss : 5.918397426605225: 100%|██████████| 125/125 [00:09<00:00, 12.57it/s] \n",
      "epoch:39 loss : 5.8001227378845215: 100%|██████████| 125/125 [00:11<00:00, 11.25it/s]\n",
      "epoch:40 loss : 5.675137519836426: 100%|██████████| 125/125 [00:10<00:00, 11.93it/s] \n",
      "epoch:41 loss : 5.7052202224731445: 100%|██████████| 125/125 [00:10<00:00, 11.86it/s]\n",
      "epoch:42 loss : 5.641032695770264: 100%|██████████| 125/125 [00:10<00:00, 12.20it/s] \n",
      "epoch:43 loss : 5.654796123504639: 100%|██████████| 125/125 [00:09<00:00, 12.68it/s] \n",
      "epoch:44 loss : 5.620891571044922: 100%|██████████| 125/125 [00:12<00:00, 10.21it/s] \n",
      "epoch:45 loss : 5.608973503112793: 100%|██████████| 125/125 [00:11<00:00, 10.86it/s] \n",
      "epoch:46 loss : 5.567655086517334: 100%|██████████| 125/125 [00:09<00:00, 12.53it/s] \n",
      "epoch:47 loss : 5.3761396408081055: 100%|██████████| 125/125 [00:09<00:00, 12.59it/s]\n",
      "epoch:48 loss : 5.456446647644043: 100%|██████████| 125/125 [00:09<00:00, 12.74it/s] \n",
      "epoch:49 loss : 5.420070648193359: 100%|██████████| 125/125 [00:09<00:00, 12.53it/s] \n",
      "epoch:50 loss : 5.407680988311768: 100%|██████████| 125/125 [00:09<00:00, 13.00it/s] \n",
      "epoch:51 loss : 5.317891597747803: 100%|██████████| 125/125 [00:10<00:00, 12.35it/s] \n",
      "epoch:52 loss : 5.111366271972656: 100%|██████████| 125/125 [00:10<00:00, 12.47it/s] \n",
      "epoch:53 loss : 5.296587944030762: 100%|██████████| 125/125 [00:10<00:00, 11.78it/s] \n",
      "epoch:54 loss : 5.139300346374512: 100%|██████████| 125/125 [00:10<00:00, 11.39it/s] \n",
      "epoch:55 loss : 5.101860523223877: 100%|██████████| 125/125 [00:10<00:00, 12.22it/s] \n",
      "epoch:56 loss : 5.041581153869629: 100%|██████████| 125/125 [00:11<00:00, 11.33it/s] \n",
      "epoch:57 loss : 4.934971809387207: 100%|██████████| 125/125 [00:10<00:00, 11.50it/s] \n",
      "epoch:58 loss : 4.921231269836426: 100%|██████████| 125/125 [00:10<00:00, 11.49it/s] \n",
      "epoch:59 loss : 4.844514846801758: 100%|██████████| 125/125 [00:12<00:00, 10.39it/s] \n",
      "epoch:60 loss : 4.987355709075928: 100%|██████████| 125/125 [00:11<00:00, 10.46it/s] \n",
      "epoch:61 loss : 5.007025241851807: 100%|██████████| 125/125 [00:09<00:00, 12.50it/s] \n",
      "epoch:62 loss : 4.9279680252075195: 100%|██████████| 125/125 [00:09<00:00, 13.10it/s]\n",
      "epoch:63 loss : 4.870734214782715: 100%|██████████| 125/125 [00:09<00:00, 13.05it/s] \n",
      "epoch:64 loss : 4.8419976234436035: 100%|██████████| 125/125 [00:09<00:00, 12.73it/s]\n",
      "epoch:65 loss : 4.763208866119385: 100%|██████████| 125/125 [00:09<00:00, 12.75it/s] \n",
      "epoch:66 loss : 4.83922004699707: 100%|██████████| 125/125 [00:09<00:00, 12.99it/s]  \n",
      "epoch:67 loss : 4.887927055358887: 100%|██████████| 125/125 [00:10<00:00, 12.33it/s] \n",
      "epoch:68 loss : 4.741518497467041: 100%|██████████| 125/125 [00:10<00:00, 12.33it/s] \n",
      "epoch:69 loss : 4.807513236999512: 100%|██████████| 125/125 [00:09<00:00, 12.54it/s] \n",
      "epoch:70 loss : 4.763250827789307: 100%|██████████| 125/125 [00:09<00:00, 13.16it/s] \n",
      "epoch:71 loss : 4.919040203094482: 100%|██████████| 125/125 [00:09<00:00, 12.59it/s] \n",
      "epoch:72 loss : 4.8427605628967285: 100%|██████████| 125/125 [00:10<00:00, 11.85it/s]\n",
      "epoch:73 loss : 4.819324493408203: 100%|██████████| 125/125 [00:11<00:00, 10.92it/s] \n",
      "epoch:74 loss : 4.934329032897949: 100%|██████████| 125/125 [00:10<00:00, 12.04it/s] \n",
      "epoch:75 loss : 4.678778648376465: 100%|██████████| 125/125 [00:10<00:00, 11.74it/s] \n",
      "epoch:76 loss : 4.6760172843933105: 100%|██████████| 125/125 [00:10<00:00, 11.84it/s]\n",
      "epoch:77 loss : 4.747298717498779: 100%|██████████| 125/125 [00:12<00:00, 10.14it/s] \n",
      "epoch:78 loss : 4.721282005310059: 100%|██████████| 125/125 [00:12<00:00,  9.64it/s] \n",
      "epoch:79 loss : 4.409859657287598: 100%|██████████| 125/125 [00:12<00:00, 10.24it/s] \n",
      "epoch:80 loss : 4.419713497161865: 100%|██████████| 125/125 [00:11<00:00, 11.23it/s] \n",
      "epoch:81 loss : 4.410383701324463: 100%|██████████| 125/125 [00:11<00:00, 11.24it/s] \n",
      "epoch:82 loss : 4.624228000640869: 100%|██████████| 125/125 [00:11<00:00, 11.11it/s] \n",
      "epoch:83 loss : 4.422795295715332: 100%|██████████| 125/125 [00:10<00:00, 12.44it/s] \n",
      "epoch:84 loss : 4.445849895477295: 100%|██████████| 125/125 [00:11<00:00, 11.24it/s] \n",
      "epoch:85 loss : 4.421515941619873: 100%|██████████| 125/125 [00:15<00:00,  8.17it/s] \n",
      "epoch:86 loss : 4.356750011444092: 100%|██████████| 125/125 [00:11<00:00, 11.35it/s] \n",
      "epoch:87 loss : 4.261406421661377: 100%|██████████| 125/125 [00:09<00:00, 12.52it/s] \n",
      "epoch:88 loss : 4.257054328918457: 100%|██████████| 125/125 [00:09<00:00, 12.73it/s] \n",
      "epoch:89 loss : 4.372184753417969: 100%|██████████| 125/125 [00:10<00:00, 11.66it/s] \n",
      "epoch:90 loss : 4.489079475402832: 100%|██████████| 125/125 [00:11<00:00, 11.01it/s] \n",
      "epoch:91 loss : 4.392056941986084: 100%|██████████| 125/125 [00:11<00:00, 11.30it/s] \n",
      "epoch:92 loss : 3.972898006439209: 100%|██████████| 125/125 [00:10<00:00, 11.66it/s] \n",
      "epoch:93 loss : 3.9632325172424316: 100%|██████████| 125/125 [00:10<00:00, 11.87it/s]\n",
      "epoch:94 loss : 3.954047441482544: 100%|██████████| 125/125 [00:10<00:00, 12.26it/s] \n",
      "epoch:95 loss : 3.8903586864471436: 100%|██████████| 125/125 [00:09<00:00, 13.04it/s]\n",
      "epoch:96 loss : 3.8478775024414062: 100%|██████████| 125/125 [00:10<00:00, 12.24it/s]\n",
      "epoch:97 loss : 3.85624623298645: 100%|██████████| 125/125 [00:10<00:00, 12.35it/s]  \n",
      "epoch:98 loss : 3.909170627593994: 100%|██████████| 125/125 [00:10<00:00, 12.41it/s] \n",
      "epoch:99 loss : 3.862534999847412: 100%|██████████| 125/125 [00:10<00:00, 11.56it/s] \n",
      "epoch:100 loss : 3.929597854614258: 100%|██████████| 125/125 [00:11<00:00, 11.08it/s] \n",
      "epoch:101 loss : 3.971831798553467: 100%|██████████| 125/125 [00:10<00:00, 12.35it/s] \n",
      "epoch:102 loss : 3.900562286376953: 100%|██████████| 125/125 [00:11<00:00, 10.91it/s] \n",
      "epoch:103 loss : 3.720162868499756: 100%|██████████| 125/125 [00:10<00:00, 11.98it/s] \n",
      "epoch:104 loss : 3.6347739696502686: 100%|██████████| 125/125 [00:11<00:00, 10.72it/s]\n",
      "epoch:105 loss : 3.3948917388916016: 100%|██████████| 125/125 [00:11<00:00, 11.22it/s]\n",
      "epoch:106 loss : 3.2045769691467285: 100%|██████████| 125/125 [00:09<00:00, 13.00it/s]\n",
      "epoch:107 loss : 3.274139881134033: 100%|██████████| 125/125 [00:10<00:00, 11.77it/s] \n",
      "epoch:108 loss : 3.344641923904419: 100%|██████████| 125/125 [00:10<00:00, 11.56it/s] \n",
      "epoch:109 loss : 3.431628465652466: 100%|██████████| 125/125 [00:10<00:00, 12.06it/s] \n",
      "epoch:110 loss : 3.4228432178497314: 100%|██████████| 125/125 [00:10<00:00, 11.94it/s]\n",
      "epoch:111 loss : 3.5460281372070312: 100%|██████████| 125/125 [00:10<00:00, 11.70it/s]\n",
      "epoch:112 loss : 3.571667194366455: 100%|██████████| 125/125 [00:10<00:00, 11.93it/s] \n",
      "epoch:113 loss : 3.401078462600708: 100%|██████████| 125/125 [00:11<00:00, 11.02it/s] \n",
      "epoch:114 loss : 3.310337781906128: 100%|██████████| 125/125 [00:10<00:00, 12.24it/s] \n",
      "epoch:115 loss : 3.16953706741333: 100%|██████████| 125/125 [00:11<00:00, 10.92it/s]  \n",
      "epoch:116 loss : 3.1421971321105957: 100%|██████████| 125/125 [00:11<00:00, 10.95it/s]\n",
      "epoch:117 loss : 3.1216418743133545: 100%|██████████| 125/125 [00:10<00:00, 11.40it/s]\n",
      "epoch:118 loss : 2.8466451168060303: 100%|██████████| 125/125 [00:09<00:00, 12.70it/s]\n",
      "epoch:119 loss : 2.8002943992614746: 100%|██████████| 125/125 [00:10<00:00, 11.52it/s]\n",
      "epoch:120 loss : 2.8116486072540283: 100%|██████████| 125/125 [00:10<00:00, 12.16it/s]\n",
      "epoch:121 loss : 2.868346691131592: 100%|██████████| 125/125 [00:11<00:00, 11.25it/s] \n",
      "epoch:122 loss : 3.1016101837158203: 100%|██████████| 125/125 [00:10<00:00, 11.72it/s]\n",
      "epoch:123 loss : 3.1347923278808594: 100%|██████████| 125/125 [00:11<00:00, 10.65it/s]\n",
      "epoch:124 loss : 3.1367485523223877: 100%|██████████| 125/125 [00:10<00:00, 11.65it/s]\n",
      "epoch:125 loss : 3.1408493518829346: 100%|██████████| 125/125 [00:10<00:00, 11.75it/s]\n",
      "epoch:126 loss : 3.3061206340789795: 100%|██████████| 125/125 [00:10<00:00, 11.51it/s]\n",
      "epoch:127 loss : 2.8434433937072754: 100%|██████████| 125/125 [00:11<00:00, 11.29it/s]\n",
      "epoch:128 loss : 3.81996488571167:  40%|████      | 50/125 [00:04<00:06, 12.00it/s]  "
     ]
    }
   ],
   "source": [
    "# 모델....\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset = TextGeneration(csv_lists)\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset,batch_size=32)\n",
    "optim = Adam(model.parameters(), lr = 1e-3)\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data, label in loop:\n",
    "        data,label = data.to(device), label.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred = model(torch.tensor(data, dtype=torch.long))\n",
    "        loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'epoch:{epoch+1} loss : {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 예측\n",
    "# 입력문 장을 텐서로 변경  임베딩 벡터  Bow를 이용해서 \n",
    "sample = \"i love\"\n",
    "with torch.no_grad():\n",
    "    words = torch.tensor(\n",
    "       [dataset.BOW[w]  for w in sample.split()],dtype=torch.long\n",
    "    ).to(device).unsqueeze(0)\n",
    "    # (2,) -- > (batch, sq_len)  (1,2)\n",
    "\n",
    "    output = model(words)\n",
    "\n",
    "    # 출력은 단어의개수 만큼  len(BOW)  (batch, len(BOW))\n",
    "    # 확률이 가장 높은 단어 찾기\n",
    "    predicted_index = torch.argmax(output,dim=1).item()\n",
    "\n",
    "    # 단어사전 BOW에서 인덱스에 해당하는 단어를 찾기\n",
    "    # 역 dict를 만들어서 찾기\n",
    "      \n",
    "    reverse_bow = {value:key for key,value in dataset.BOW.items()}\n",
    "    predicted_word = reverse_bow[predicted_index]   \n",
    "    print(predicted_word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
